{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a1436ff-ca6f-4501-8f24-933fd7879545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 2.2.3\n",
      "NumPy version: 2.0.2\n",
      "Scikit-learn version: 1.5.2\n"
     ]
    }
   ],
   "source": [
    "# Retail Sales Data Analysis and Exploration\n",
    "## Project Overview\n",
    "\"\"\"\n",
    "This notebook contains the initial exploration of retail sales data for forecasting purposes. The analysis focuses on understanding \n",
    "data patterns, quality, and potential features for our forecasting model.    \n",
    "\"\"\"\n",
    "# Step 1: Import all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn  # Add this basic import\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import plotly.express as px\n",
    "from datetime import datetime\n",
    "import plotly.graph_objs as go\n",
    "import plotly.subplots as sp\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "# Verify your setup - now this will work!\n",
    "print(f\"Python version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4140590d-d77f-44c7-b22f-e0c29f18d1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /xxxxxxxxx\n",
      "Data loaded successfully\n",
      "Missing Value Analysis\n",
      "--------------------------------------------------\n",
      "                   Missing Values  Percentage Missing\n",
      "CustomerID                   4978                10.0\n",
      "ShippingCost                 2489                 5.0\n",
      "WarehouseLocation            3485                 7.0\n",
      "\n",
      "Duplicate Records Analysis\n",
      "--------------------------------------------------\n",
      "Number of duplicate records: 0\n",
      "\n",
      "Missing Values After Cleaning\n",
      "--------------------------------------------------\n",
      "InvoiceNo            0\n",
      "StockCode            0\n",
      "Description          0\n",
      "Quantity             0\n",
      "InvoiceDate          0\n",
      "UnitPrice            0\n",
      "CustomerID           0\n",
      "Country              0\n",
      "Discount             0\n",
      "PaymentMethod        0\n",
      "ShippingCost         0\n",
      "Category             0\n",
      "SalesChannel         0\n",
      "ReturnStatus         0\n",
      "ShipmentProvider     0\n",
      "WarehouseLocation    0\n",
      "OrderPriority        0\n",
      "dtype: int64\n",
      "Full path of saved file: /xxxxxxxxx"
     ]
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "print(f\"Current working directory: {current_dir}\")\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load the retail sales dataset and perform initial datetime conversion.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv('/xxxxxxxxxxxxx')\n",
    "    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "    return df\n",
    "\n",
    "def assess_data_quality(df):\n",
    "    \"\"\"\n",
    "    Perform initial data quality checks.\n",
    "    \"\"\"\n",
    "    print(\"Missing Value Analysis\")\n",
    "    print(\"-\" * 50)\n",
    "    missing_stats = pd.DataFrame({\n",
    "        'Missing Values': df.isnull().sum(),\n",
    "        'Percentage Missing': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "    })\n",
    "    print(missing_stats[missing_stats['Missing Values'] > 0])\n",
    "    \n",
    "    print(\"\\nDuplicate Records Analysis\")\n",
    "    print(\"-\" * 50)\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"Number of duplicate records: {duplicates:,}\")\n",
    "    \n",
    "    return missing_stats\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    \"\"\"\n",
    "    Handle missing values in the retail dataset using specific strategies.\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Fill WarehouseLocation with mode\n",
    "    df_clean['WarehouseLocation'] = df_clean['WarehouseLocation'].fillna(\n",
    "        df_clean['WarehouseLocation'].mode()[0]\n",
    "    )\n",
    "    \n",
    "    # Fill ShippingCost with country-wise median\n",
    "    df_clean['ShippingCost'] = df_clean.groupby('Country')['ShippingCost'].transform(\n",
    "        lambda x: x.fillna(x.median())\n",
    "    )\n",
    "\n",
    "    # Generate random guest customer IDs starting from 90000\n",
    "    # This ensures they're distinguishable from regular customer IDs\n",
    "    missing_customer_count = df_clean['CustomerID'].isnull().sum()\n",
    "    guest_ids = np.arange(90000, 90000 + missing_customer_count)\n",
    "    \n",
    "    # Fill missing CustomerIDs with these guest IDs\n",
    "    df_clean.loc[df_clean['CustomerID'].isnull(), 'CustomerID'] = guest_ids\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Main execution sequence\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Load the data\n",
    "    df = load_data()\n",
    "    print(\"Data loaded successfully\")\n",
    "    \n",
    "    # 2. Assess data quality\n",
    "    missing_stats = assess_data_quality(df)\n",
    "    \n",
    "    # 3. Handle missing values\n",
    "    df_clean = handle_missing_values(df)\n",
    "    \n",
    "    # 4. Verify cleaning results\n",
    "    print(\"\\nMissing Values After Cleaning\")\n",
    "    print(\"-\" * 50)\n",
    "    print(df_clean.isnull().sum())\n",
    "    \n",
    "    # 5. Save processed dataset\n",
    "    output_dir = os.path.join(current_dir, 'data', 'processed')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, 'retail_sales_2024_cleaned.csv')\n",
    "    df_clean.to_csv(output_path, index=False)\n",
    "    print(f\"Full path of saved file: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "380be175-caf4-4970-8030-bbb08a901e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cleaned_data():\n",
    "    \"\"\"\n",
    "    Load the cleaned retail dataset and perform datetime conversion.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned dataset with converted datetime\n",
    "    \"\"\"\n",
    "    cleaned_path = 'xxxxxxxxxx'\n",
    "    df = pd.read_csv(cleaned_path)\n",
    "    df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53ea29ac-31f3-4011-b819-4fc61c45055a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n",
      "All tables created successfully\n",
      "\n",
      "Optimized tables created for Tableau analysis:\n",
      "\n",
      "fact_sales.csv - Core transaction data:\n",
      "- Contains all numerical metrics and high-cardinality identifiers\n",
      "- Enables detailed transaction-level analysis\n",
      "\n",
      "dim_product.csv - Product information:\n",
      "- Links products to their descriptions and categories\n",
      "- Supports product hierarchy analysis\n",
      "\n",
      "dim_customer.csv - Customer details:\n",
      "- Contains customer-related attributes\n",
      "- Enables customer segmentation analysis\n",
      "\n",
      "dim_shipment.csv - Shipment and order details:\n",
      "- Consolidates all shipping-related attributes\n",
      "- Supports order fulfillment analysis\n",
      "\n",
      "dim_date.csv - Time dimension:\n",
      "- Provides hierarchical time attributes\n",
      "- Enables temporal pattern analysis\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def load_cleaned_data(file_path):\n",
    "    \"\"\"\n",
    "    Load and validate the cleaned retail dataset.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the cleaned CSV file\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded and validated dataframe\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "        print(\"Data loaded successfully\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def create_optimized_tables():\n",
    "    \"\"\"\n",
    "    Create optimized fact and dimension tables based on cardinality analysis.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load cleaned data\n",
    "        input_path = 'xxxxxxxxx'\n",
    "        df = load_cleaned_data(input_path)\n",
    "        \n",
    "        # Create fact table focusing on numerical metrics\n",
    "        fact_sales = df[[\n",
    "            'InvoiceNo',\n",
    "            'StockCode',\n",
    "            'CustomerID',\n",
    "            'InvoiceDate',\n",
    "            'Quantity',\n",
    "            'UnitPrice',\n",
    "            'Discount',\n",
    "            'ShippingCost'\n",
    "        ]].copy()\n",
    "        \n",
    "        # Calculate total amount with discount\n",
    "        fact_sales['TotalAmount'] = (fact_sales['Quantity'] * \n",
    "                                   fact_sales['UnitPrice'] * \n",
    "                                   (1 - fact_sales['Discount']))\n",
    "        \n",
    "        # Format InvoiceDate consistently\n",
    "        fact_sales['InvoiceDate'] = fact_sales['InvoiceDate'].dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Create dimension tables\n",
    "        dim_product = df[[\n",
    "            'StockCode',\n",
    "            'Description',\n",
    "            'Category'\n",
    "        ]].drop_duplicates()\n",
    "        \n",
    "        dim_customer = df[[\n",
    "            'CustomerID',\n",
    "            'Country',\n",
    "            'SalesChannel'\n",
    "        ]].drop_duplicates()\n",
    "        \n",
    "        dim_shipment = df[[\n",
    "            'InvoiceNo',\n",
    "            'ShipmentProvider',\n",
    "            'WarehouseLocation',\n",
    "            'OrderPriority',\n",
    "            'ReturnStatus',\n",
    "            'PaymentMethod'\n",
    "        ]].drop_duplicates()\n",
    "        \n",
    "        # Create date dimension with matching format\n",
    "        dim_date = pd.DataFrame({\n",
    "            'InvoiceDate': df['InvoiceDate'].dt.strftime('%Y-%m-%d').unique()\n",
    "        })\n",
    "        \n",
    "        # Add date attributes\n",
    "        dim_date['DateForCalculation'] = pd.to_datetime(dim_date['InvoiceDate'])\n",
    "        dim_date['Year'] = dim_date['DateForCalculation'].dt.year\n",
    "        dim_date['Month'] = dim_date['DateForCalculation'].dt.month\n",
    "        dim_date['Quarter'] = dim_date['DateForCalculation'].dt.quarter\n",
    "        dim_date['DayOfWeek'] = dim_date['DateForCalculation'].dt.dayofweek\n",
    "        dim_date['IsWeekend'] = dim_date['DayOfWeek'].isin([5, 6]).astype(int)\n",
    "        dim_date = dim_date.drop('DateForCalculation', axis=1)\n",
    "        \n",
    "        # Save all tables\n",
    "        output_dir = 'xxxxxxx'\n",
    "        \n",
    "        fact_sales.to_csv(f'{output_dir}/fact_sales.csv', index=False)\n",
    "        dim_product.to_csv(f'{output_dir}/dim_product.csv', index=False)\n",
    "        dim_customer.to_csv(f'{output_dir}/dim_customer.csv', index=False)\n",
    "        dim_shipment.to_csv(f'{output_dir}/dim_shipment.csv', index=False)\n",
    "        dim_date.to_csv(f'{output_dir}/dim_date.csv', index=False)\n",
    "        \n",
    "        print(\"All tables created successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating tables: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Execute the creation of optimized tables and provide documentation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create output directory\n",
    "        output_dir = 'xxxxxxxx'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Create all tables\n",
    "        create_optimized_tables()\n",
    "        \n",
    "        # Print documentation\n",
    "        print(\"\\nOptimized tables created for Tableau analysis:\")\n",
    "        print(\"\\nfact_sales.csv - Core transaction data:\")\n",
    "        print(\"- Contains all numerical metrics and high-cardinality identifiers\")\n",
    "        print(\"- Enables detailed transaction-level analysis\")\n",
    "        \n",
    "        print(\"\\ndim_product.csv - Product information:\")\n",
    "        print(\"- Links products to their descriptions and categories\")\n",
    "        print(\"- Supports product hierarchy analysis\")\n",
    "        \n",
    "        print(\"\\ndim_customer.csv - Customer details:\")\n",
    "        print(\"- Contains customer-related attributes\")\n",
    "        print(\"- Enables customer segmentation analysis\")\n",
    "        \n",
    "        print(\"\\ndim_shipment.csv - Shipment and order details:\")\n",
    "        print(\"- Consolidates all shipping-related attributes\")\n",
    "        print(\"- Supports order fulfillment analysis\")\n",
    "        \n",
    "        print(\"\\ndim_date.csv - Time dimension:\")\n",
    "        print(\"- Provides hierarchical time attributes\")\n",
    "        print(\"- Enables temporal pattern analysis\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e544f2d0-53b8-47cc-ac2b-e51cce7ef9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
